_wandb:
    value:
        cli_version: 0.23.1
        e:
            ec5e05pq68qje8284kpr2kj846cnu64x:
                args:
                    - --config
                    - config/default.yaml
                codePath: scripts/train.py
                codePathLocal: scripts/train.py
                cpu_count: 288
                cpu_count_logical: 288
                cudaVersion: "12.6"
                disk:
                    /:
                        total: "256871432192"
                        used: "28815785984"
                email: lhy@stanford.edu
                executable: /sw/user/python/miniforge3-pytorch-2.5.0/bin/python3.10
                gpu: NVIDIA GH200 120GB
                gpu_count: 4
                gpu_nvidia:
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "102625181696"
                      name: NVIDIA GH200 120GB
                      uuid: GPU-8d01b693-3b09-ee3f-fc4c-bc63c16cd8ec
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "102625181696"
                      name: NVIDIA GH200 120GB
                      uuid: GPU-474f4a25-aa3a-1a2e-9941-accb68bbc9c7
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "102625181696"
                      name: NVIDIA GH200 120GB
                      uuid: GPU-ee2ce61d-525c-088a-3fb7-f2ec967127ce
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "102625181696"
                      name: NVIDIA GH200 120GB
                      uuid: GPU-a1dba8d8-cd5b-a91d-3860-b30013e0cafe
                host: gh039.hsn.cm.delta.internal.ncsa.edu
                memory:
                    total: "919617339392"
                os: Linux-5.14.21-150500.55.65_13.0.73-cray_shasta_c_64k-aarch64-with-glibc2.31
                program: /u/hli48/sft/scripts/train.py
                python: CPython 3.10.14
                root: /u/hli48/sft
                slurm:
                    cluster_name: delta-gh
                    conf: /var/spool/slurmd/conf-cache/slurm.conf
                    cpus_on_node: "16"
                    cpus_per_task: "16"
                    export_env: NONE
                    get_user_env: "1"
                    gpus_on_node: "4"
                    gpus_per_node: "4"
                    gtids: "0"
                    job_account: bfyv-dtai-gh
                    job_cpus_per_node: "16"
                    job_end_time: "1767964555"
                    job_gid: "202"
                    job_gpus: 0,1,2,3
                    job_id: "1792777"
                    job_name: llm_rl
                    job_nodelist: gh039
                    job_num_nodes: "1"
                    job_partition: ghx4
                    job_qos: bfyv-dtai-gh
                    job_start_time: "1767892555"
                    job_uid: "91813"
                    job_user: hli48
                    jobid: "1792777"
                    localid: "0"
                    mem_per_node: "307200"
                    nnodes: "1"
                    nodeid: "0"
                    nodelist: gh039
                    nprocs: "1"
                    ntasks: "1"
                    ntasks_per_node: "1"
                    oom_kill_step: "0"
                    prio_process: "0"
                    procid: "0"
                    submit_dir: /u/hli48/sft
                    submit_host: gh089.hsn.cm.delta.internal.ncsa.edu
                    task_pid: "803552"
                    tasks_per_node: "1"
                    topology_addr: deltaai_fabric.chassis_x8101c2.gh039
                    topology_addr_pattern: switch.switch.node
                    tres_per_task: cpu=16
                startedAt: "2026-01-08T17:18:31.772829Z"
                writerId: ec5e05pq68qje8284kpr2kj846cnu64x
        m: []
        python_version: 3.10.14
        t:
            "1":
                - 1
                - 11
                - 41
                - 49
                - 51
                - 71
                - 105
            "2":
                - 1
                - 11
                - 41
                - 49
                - 51
                - 71
                - 105
            "3":
                - 2
                - 13
                - 16
            "4": 3.10.14
            "5": 0.23.1
            "6": 4.57.3
            "12": 0.23.1
            "13": linux-aarch64
dataset:
    value:
        answer_column: answer
        max_length: 2048
        name: KbsdJames/Omni-MATH
        problem_column: problem
        solution_column: solution
        split: test
        train_test_split: 0.95
fsdp:
    value:
        cpu_offload: false
        enabled: true
        sharding_strategy: FULL_SHARD
        state_dict_type: FULL_STATE_DICT
model:
    value:
        name: Qwen/Qwen3-4B-Instruct-2507
        torch_dtype: bfloat16
        trust_remote_code: true
        use_flash_attention: true
training:
    value:
        batch_size_per_device: 2
        eval_steps: 500
        gradient_accumulation_steps: 4
        learning_rate: 2e-05
        logging_steps: 10
        lr_scheduler_type: cosine
        max_grad_norm: 1
        num_epochs: 3
        output_dir: /projects/bfyv/hli48/sft_checkpoints
        save_steps: 500
        save_total_limit: 3
        seed: 42
        warmup_steps: 100
        weight_decay: 0.01
wandb:
    value:
        enabled: true
        project: llm_rl
        run_name: null
